{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fda46f4c-8075-4260-9541-f1973789d39a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Dit script analyseert de verdeling van elke kolom in een DataFrame om de best passende distributie te vinden. Voor elke kolom worden NaN-waarden en waarden gelijk aan '0' uitgesloten van de analyse. Vervolgens worden verschillende veelvoorkomende statistische distributies, zoals normaal, lognormaal, exponentieel en gamma, aangepast aan de gegevens van elke kolom. De empirische verdeling van de gegevens wordt geplot samen met de geschatte verdelingscurves. Dit stelt de gebruiker in staat om visueel te beoordelen welke distributie het best bij de gegevens van elke kolom past. De code maakt gebruik van 'matplotlib' voor het plotten en 'scipy.stats' voor de distributiefitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6b7d4fd6-e171-49eb-8b14-9d2bc68140ae",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def analyze_distributions(df, columns, distributions):\n",
    "    for column in columns:\n",
    "        column_data = df[column].dropna()\n",
    "        column_data = column_data[column_data != 0]\n",
    "\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.hist(column_data, bins=30, density=True, alpha=0.6, color='g')\n",
    "\n",
    "        # Initialiseer variabelen om de beste fit te vinden\n",
    "        best_distribution = None\n",
    "        best_params = None\n",
    "        best_sse = np.inf\n",
    "\n",
    "        for distribution in distributions:\n",
    "            params = distribution.fit(column_data)\n",
    "\n",
    "            x = np.linspace(min(column_data), max(column_data), 100)\n",
    "            plt.plot(x, distribution.pdf(x, *params), label=f'{distribution.name}')\n",
    "\n",
    "            # Bereken de sum of squared errors\n",
    "            estimated_pdf = distribution.pdf(column_data, *params)\n",
    "            sse = np.sum(np.power(column_data - estimated_pdf, 2))\n",
    "\n",
    "            if sse < best_sse:\n",
    "                best_sse = sse\n",
    "                best_distribution = distribution.name\n",
    "                best_params = params\n",
    "\n",
    "        plt.title(f'Data versus Distributies voor {column}')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "        # Print de samenvatting van de beste fit\n",
    "        print(f\"Beste fit voor {column}: {best_distribution}\")\n",
    "        print(f\"  Parameters: {best_params}\")\n",
    "        print(f\"  SSE: {best_sse}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "212cc83e-549c-4dde-88fa-cf9140eefed0",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Dit script is ontworpen om de lineaire relatie tussen twee specifieke kolommen in een DataFrame te analyseren en te visualiseren. Het focust op de relatie tussen een doelkolom ('target_column') en een vergelijkingskolom ('comparison_column'). De functie plot_linear_relationships verwijdert eerst alle NaN-waarden uit beide kolommen, zorgt ervoor dat ze dezelfde lengte hebben, en maakt vervolgens een scatterplot met een lineaire regressielijn. Daarnaast wordt een samenvatting gegeven die de eigenschappen van de lineaire relatie beschrijft, waaronder de helling, het intercept, de correlatiecoëfficiënt (R-waarde) en de p-waarde. De functie geeft ook een waardeoordeel gebaseerd op de sterkte en significantie van de correlatie, waardoor inzicht wordt verkregen in de aard van de relatie tussen de twee kolommen. Door middel van de 'hue' kan een derde variabelen worden toegevoegd aan de plot, waardoor de gebruiker de relatie tussen de twee kolommen kan visualiseren voor verschillende waarden van de derde variabele."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0d7ea17a-5da2-4d5b-a1bd-37df9375bb2f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def plot_linear_relationships(df, target_column, comparison_column, hue_column=None):\n",
    "    plt.figure(figsize=(15, 12))\n",
    "\n",
    "    # Bereid de data voor door alleen rijen te behouden waar beide kolommen geldige waarden hebben\n",
    "    valid_data = df[[target_column, comparison_column]].dropna()\n",
    "\n",
    "    if hue_column and hue_column in df.columns:\n",
    "        valid_data = valid_data.join(df[hue_column])\n",
    "\n",
    "    x = valid_data[target_column]\n",
    "    y = valid_data[comparison_column]\n",
    "\n",
    "    # Bereken de lineaire regressielijn\n",
    "    slope, intercept, r_value, p_value, std_err = stats.linregress(x, y)\n",
    "\n",
    "    # Maak de scatterplot met hue als het is meegegeven\n",
    "    if hue_column and hue_column in df.columns:\n",
    "        sns.scatterplot(x=x, y=y, hue=valid_data[hue_column] ,palette='viridis')\n",
    "    else:\n",
    "        plt.scatter(x, y, alpha=0.5, label=comparison_column)\n",
    "\n",
    "    # Voeg de regressielijn toe\n",
    "    plt.plot(x, intercept + slope * x, 'r', label=f'y={slope:.2f}x+{intercept:.2f}')\n",
    "\n",
    "    # Voeg titels en legenda toe\n",
    "    plt.title(f'Lineaire Relatie tussen {target_column} en {comparison_column}')\n",
    "    plt.xlabel(target_column)\n",
    "    plt.ylabel(comparison_column)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    # Print de samenvatting van de relatie\n",
    "    print(f\"Samenvatting voor {comparison_column}:\")\n",
    "    print(f\"  Helling (Slope): {slope:.2f}\")\n",
    "    print(f\"  Intercept: {intercept:.2f}\")\n",
    "    print(f\"  Correlatiecoëfficiënt (R-waarde): {r_value:.2f}\")\n",
    "    print(f\"  P-waarde: {p_value:.2f}\")\n",
    "    \n",
    "    # Waardeoordeel gebaseerd op R-waarde en P-waarde\n",
    "    if p_value < 0.05:\n",
    "        if abs(r_value) > 0.7:\n",
    "            print(\"  Sterke en significante relatie.\")\n",
    "        elif abs(r_value) > 0.3:\n",
    "            print(\"  Matige, maar significante relatie.\")\n",
    "        else:\n",
    "            print(\"  Zwakke, maar significante relatie.\")\n",
    "    else:\n",
    "        print(\"  Geen significante relatie.\")\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "242ea9de-c153-43ca-b0b4-49a6559ee999",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Deze functie, plot_corr, is ontwikkeld om een correlatieheatmap te genereren voor een gegeven pandas DataFrame. De functie biedt de flexibiliteit om een specifieke subset van kolommen (cols) te selecteren voor de correlatieanalyse. Als geen kolommen worden opgegeven, analyseert de functie alle kolommen in het DataFrame. De heatmap visualiseert de Pearson-correlatiecoëfficiënten tussen de kolommen, waarbij sterkere correlaties worden weergegeven in meer intense kleuren. De functie maakt gebruik van Seaborn's heatmap met een divergerende kleurenpalet voor duidelijke visualisatie van positieve en negatieve correlaties. Parameters zoals vmax (vm) bieden controle over de kleurenschaal van de heatmap, waarmee de gebruiker kan bepalen hoe correlatiewaarden worden gemapt naar de kleurenschaal. Deze functie is bijzonder nuttig voor het verkennen van relaties tussen variabelen in een dataset, wat essentieel is voor data-analyse en feature selectie processen.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2bdff5bd-9022-4d9f-8df5-46e14474d96c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def plot_corr(df, cols=None, vm=1.0):\n",
    "    # Creëert een nieuwe figuur en assen voor de plot\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    # Als geen specifieke kolommen zijn opgegeven, gebruik dan alle kolommen in het DataFrame\n",
    "    if cols is None:\n",
    "        cols = df.columns\n",
    "\n",
    "    # Bereken de correlatiematrix van de geselecteerde kolommen\n",
    "    corr = df[cols].corr()\n",
    "\n",
    "    # Maak een masker voor het bovenste driehoekige deel van de heatmap, om duplicatie te voorkomen\n",
    "    mask = np.zeros_like(corr, dtype=bool)\n",
    "    mask[np.triu_indices_from(mask)] = True\n",
    "\n",
    "    # Definieer een kleurenpalet voor de heatmap\n",
    "    cmap = sns.diverging_palette(220, 10, as_cmap=True)\n",
    "\n",
    "    # Creëer de heatmap van de correlatiematrix\n",
    "    sns.heatmap(corr, \n",
    "                #mask=mask,  # Optie om het masker toe te passen om alleen het onderste driehoekige deel te tonen\n",
    "                cmap=cmap,  # Het gekozen kleurenpalet\n",
    "                fmt=\".2f\",  # Format voor de getallen in de cellen\n",
    "                vmax=vm,    # Maximumwaarde voor de kleurenschaal\n",
    "                vmin=-vm,   # Minimumwaarde voor de kleurenschaal\n",
    "                center=0,   # Centrumwaarde voor de kleurenschaal, waarbij wit meestal de kleur van het midden is\n",
    "                square=True,  # Zorgt ervoor dat elke cel vierkant is\n",
    "                linewidths=.5,  # Breedte van de lijnen die de cellen scheiden\n",
    "                cbar_kws={\"shrink\": .5},  # Extra opties voor de legenda, hier wordt het met 50% verkleind\n",
    "                ax=ax  # De assen waarop de heatmap geplot wordt\n",
    "               )\n",
    "\n",
    "    # Retourneer de correlatiematrix en de assen\n",
    "    return corr, ax\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a83b82e6-8395-43f6-86c6-5392131cb009",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Deze functie, analyze_data_distribution, is ontworpen om de verdeling van data in specifieke kolommen van een pandas DataFrame te analyseren. Voor elke opgegeven kolom worden eerst NaN-waarden en waarden gelijk aan '0' verwijderd. Vervolgens genereert de functie voor elke kolom een histogram en een Quantile-Quantile (QQ) plot om visueel te beoordelen of de gegevens een normale verdeling volgen. Daarnaast voert de functie een Shapiro-Wilk test uit om de normaliteit van de gegevens statistisch te toetsen. Deze test is beperkt tot 5000 waarnemingen per kolom; indien een kolom meer waarnemingen bevat, wordt een steekproef genomen. De functie is nuttig voor het preliminair onderzoeken van de gegevensdistributies in een dataset, wat cruciaal kan zijn voor verdere data-analyse en modelselectie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aeb4438c-d7c2-412c-96d2-7823b582ee97",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def analyze_data_distribution(df, headers, min_value=-0.5, max_value=0.5):\n",
    "    for column in headers:\n",
    "        # Verwijder NaN waarden en waarden die gelijk zijn aan '0' uit de kolom voor de analyse\n",
    "        column_data = df[column].dropna()\n",
    "        column_data = column_data[(column_data < min_value) | (column_data > max_value)]\n",
    "\n",
    "        # Histogram\n",
    "        plt.figure(figsize=(10, 4))\n",
    "        plt.subplot(1, 2, 1)\n",
    "        column_data.hist(bins=30)\n",
    "        plt.title(f'Histogram van {column}')\n",
    "\n",
    "        # QQ-plot\n",
    "        plt.subplot(1, 2, 2)\n",
    "        stats.probplot(column_data, dist=\"norm\", plot=plt)\n",
    "        plt.title(f'QQ-plot van {column}')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        # Shapiro-Wilk Test\n",
    "        # Opmerking: de Shapiro-Wilk test is beperkt tot 5000 waarnemingen\n",
    "        if len(column_data) > 5000:\n",
    "            column_data = column_data.sample(5000)\n",
    "        stat, p = stats.shapiro(column_data)\n",
    "        print(f'Shapiro-Wilk Test voor {column}: Statistiek={stat}, p-waarde={p}')\n",
    "\n",
    "# Voorbeeld van hoe de functie te gebruiken\n",
    "# analyze_data_distribution(df, headers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "067b10fa-0ce6-4aec-9264-0381d58c2190",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Deze functie, 'describe_column', is ontworpen om essentiële statistieken en informatie te verstrekken over een specifieke kolom in een pandas DataFrame. Het geeft inzicht in de minimale en maximale waarden, evenals een overzicht van alle unieke waarden in de kolom en hun respectievelijke voorkomens. Deze functie is bijzonder nuttig voor initiële data-exploratie, waarbij een gedetailleerd begrip van individuele kolommen nodig is. Dit omvat het identificeren van het bereik van waarden, de verscheidenheid van data, en frequenties van specifieke waarden. Dergelijke informatie is cruciaal voor het bepalen van de juiste dataverwerkings- en analysestrategieën.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5eb5f760-825a-432a-b39e-4c2079225308",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def describe_column(data, column_name, bin_size=10):\n",
    "    # Vindt de minimale en maximale waarden van de gespecificeerde kolom\n",
    "    min_value = data[column_name].min()\n",
    "    max_value = data[column_name].max()\n",
    "    \n",
    "    # Toont de kolomnaam, minimum, en maximum waarden\n",
    "    print(f\"Column: {column_name}\")\n",
    "    print(f\"Minimum value: {min_value}\")\n",
    "    print(f\"Maximum value: {max_value}\")\n",
    "    \n",
    "    # Creëer de bins\n",
    "    bins = np.arange(min_value, max_value + bin_size, bin_size)\n",
    "    \n",
    "    # Classificeert de waarden in de gespecificeerde bins\n",
    "    data['Bin'] = pd.cut(data[column_name], bins=bins, include_lowest=True)\n",
    "\n",
    "    # Groepeert de data op basis van de bins en telt de waarden\n",
    "    binned_data = data.groupby('Bin')[column_name].size()\n",
    "\n",
    "    # Toont het aantal waarden in elke bin (exclusief bins met 0 waarden)\n",
    "    print(\"Number of values per bin (excluding bins with 0 values):\")\n",
    "    for bin_range, count in binned_data.items():\n",
    "        if count > 0:\n",
    "            print(f\"{bin_range}: {count} values\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function 'route_per_week' plots truck routes for each day of the week within a specified week.\n",
    "Parameters:\n",
    "\n",
    "df: DataFrame containing the truck data.\n",
    "\n",
    "start_date: String representing the start date of the week ('YYYY-MM-DD').\n",
    "\n",
    "datetime_column: The name of the column in df containing datetime information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def route_per_week(df, start_date, datetime_column='Vdatetime'):\n",
    "   \n",
    "    # Drop rows where any of these critical columns are missing\n",
    "    filtered_df = df.dropna(subset=['Vdatetime', 'GPSLatitude', 'GPSLongitude'])\n",
    "\n",
    "    # Drop rows where GPSLatitude or GPSLongitude are 0\n",
    "    filtered_df = filtered_df[(filtered_df['GPSLatitude'] > 0) | (filtered_df['GPSLongitude'] > 0)]\n",
    "    \n",
    "    # Convert start_date to datetime\n",
    "    start_date = pd.to_datetime(start_date)\n",
    "    # Calculate end_date as 7 days after start_date\n",
    "    end_date = start_date + pd.Timedelta(days=7)\n",
    "    \n",
    "    # Filter the DataFrame to the specified week\n",
    "    filtered_df = filtered_df[(filtered_df[datetime_column] >= start_date) & (filtered_df[datetime_column] <= end_date)]\n",
    "\n",
    "    # Drop rows where GPSLatitude or GPSLongitude are 0\n",
    "    filtered_df = filtered_df[(filtered_df['GPSLatitude'] > 0) | (filtered_df['GPSLongitude'] > 0)]\n",
    "    \n",
    "    # Add a 'day_of_week' column to the filtered DataFrame\n",
    "    filtered_df['day_of_week'] = filtered_df[datetime_column].dt.dayofweek\n",
    "    \n",
    "    # Set the style of the visualization\n",
    "    sns.set(style=\"darkgrid\")\n",
    "    \n",
    "    # Define a mapping from day numbers to day names\n",
    "    day_names = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "    \n",
    "    # Plotting\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    # Loop through each day of the week\n",
    "    for day in range(7):\n",
    "        # Filter the DataFrame for the day\n",
    "        day_df = filtered_df[filtered_df['day_of_week'] == day]\n",
    "        \n",
    "        # Plot, using the day name in the label\n",
    "        plt.plot(day_df['GPSLongitude'], day_df['GPSLatitude'], marker='o', linestyle='-', label=day_names[day])\n",
    "    \n",
    "    plt.title(f'RCV Routes in the Week Starting from {start_date.strftime(\"%Y-%m-%d\")}')\n",
    "    plt.xlabel('Longitude')\n",
    "    plt.xlabel('Longitude')\n",
    "    plt.ylabel('Latitude')\n",
    "    plt.legend(title='Day of Week')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Onderstaande code doet in principe het zelfde als route_per_week alleen geeft het de output in een folium map weer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def route_per_week_folium(df, start_date, datetime_column='Vdatetime'):\n",
    "\n",
    "    # Drop rows where any of these critical columns are missing\n",
    "    filtered_df = df.dropna(subset=['Vdatetime', 'GPSLatitude', 'GPSLongitude'])\n",
    "\n",
    "    # Drop rows where GPSLatitude or GPSLongitude are 0\n",
    "    filtered_df = filtered_df[(filtered_df['GPSLatitude'] > 0) | (filtered_df['GPSLongitude'] > 0)]\n",
    "\n",
    "    # Convert start_date to datetime and calculate end_date\n",
    "    start_date = pd.to_datetime(start_date)\n",
    "    end_date = start_date + pd.Timedelta(days=7)\n",
    "    \n",
    "    # Filter DataFrame for the specified week\n",
    "    filtered_df = filtered_df[(filtered_df[datetime_column] >= start_date) & (filtered_df[datetime_column] <= end_date)]\n",
    "    \n",
    "    # With 'GPSLatitude' and 'GPSLongitude'as coordinate columns\n",
    "    filtered_df['day_of_week'] = filtered_df[datetime_column].dt.dayofweek\n",
    "    \n",
    "    # Initialize a map centered around the first point in your filtered DataFrame\n",
    "    if not filtered_df.empty:\n",
    "        start_coords = (filtered_df.iloc[0]['GPSLatitude'], filtered_df.iloc[0]['GPSLongitude'])\n",
    "    else:\n",
    "        # Default to a general location if DataFrame is empty\n",
    "        start_coords = (0, 0)\n",
    "    folium_map = folium.Map(location=start_coords, zoom_start=12)\n",
    "    \n",
    "    # Define colors for each day of the week\n",
    "    colors = ['red', 'blue', 'green', 'purple', 'orange', 'darkred', 'lightblue']\n",
    "    \n",
    "    # Plot each day's route\n",
    "    for day in range(7):\n",
    "        day_df = filtered_df[filtered_df['day_of_week'] == day]\n",
    "        locations = day_df[['GPSLatitude', 'GPSLongitude']].values.tolist()\n",
    "        if locations:\n",
    "            folium.PolyLine(locations, color=colors[day], weight=2.5, opacity=1, tooltip=f'Day {day}').add_to(folium_map)\n",
    "    \n",
    "    return folium_map\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Utilities",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
